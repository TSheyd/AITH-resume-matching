{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Word2Vec\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import re\n",
    "import faiss\n",
    "\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "load additional packages for NLTK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e6bc551edaabc94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "914664e44ffea630"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rabota.ru jobs dataset was chosen for training - it's the largest one and the most diverse one probably (jobs are distributed all across Russia and IT is probably less prevalent here than on hh.ru)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd7a000403093fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vacancy.csv', sep='|')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206d52cdf73054ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edf = pd.read_csv('data/en_job_postings.csv')  # kaggle LinkedIn dataset for English data\n",
    "edf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6be460e68662bd58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing - remove HTML tags, lowercase everything, remove punctuation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449402d01ac9373a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HTML stripping (https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python)\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a16a753e7e360755"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['vac', 'sph', 'a_req', 'p_req', 'p_res']] = df[['vacancy_name', 'professionalSphereName', 'additional_requirements', 'position_requirements' , 'position_responsibilities']].astype(str)\n",
    "df['total_req'] = df['vac'] + ' ' + df['sph'] + ' ' + df['a_req'] + ' ' + df['p_req'] + ' ' + df['p_res']\n",
    "df['total_req'] = df['total_req'].str.replace('nan', '').str.strip(' ')\n",
    "df['total_req'] = df['total_req'].apply(lambda x: strip_tags(x.lower()))  # remove HTML tags, lower case\n",
    "# df['total_req'] = df['total_req'].apply(lambda x: re.sub(r'[^\\w\\s]', '', strip_tags(x.lower())))  # remove HTML tags, lower case, remove punctuation\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b10bd7dac012701"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edf[['title', 'description']] = edf[['title', 'description']].astype(str) \n",
    "edf['total_req'] = edf['title'] + ' ' + edf['description']\n",
    "edf['total_req'] = edf['total_req'].str.replace('nan', '').str.strip(' ')\n",
    "edf['total_req'] = edf['total_req'].apply(lambda x: strip_tags(x.lower()))  # remove HTML tags, lower case\n",
    "edf.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e138caceda934405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df['total_req'].values.tolist() + edf['total_req'].values.tolist()\n",
    "tagged_data = [TaggedDocument(words = nltk.tokenize.word_tokenize(_d), tags = [str(i)]) for i, _d in enumerate(data)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade2ffd4309b8daf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "init model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebe1c28f15821b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    vector_size = 80,\n",
    "    min_count = 10,\n",
    "    epochs = 70\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7538d59d9138d5d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "build vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23afb3fbf65bb20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)\n",
    "len(model.wv.key_to_index)  # vocab size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f323199f102ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b6a6306096ffb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train(tagged_data,\n",
    "total_examples = model.corpus_count,\n",
    "epochs = model.epochs)\n",
    "model.save('model/doc2vec_v4en.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddd9fa264e1025d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Embeddings for dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "252c9c8625ec787a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jobs = pd.read_csv('data/hhparser_vacancy_short.csv')\n",
    "jobs.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97bb28456b03dac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "process data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa5be970ebd0456"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jobs['content'] = jobs['name'] + ' ' + jobs['description']\n",
    "jobs['content'] = jobs['content'].astype(str)\n",
    "jobs['content'] = jobs['content'].apply(lambda x: strip_tags(x.lower()))  # Remove HTML, lower case\n",
    "# jobs['content'] = jobs['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', strip_tags(x.lower())))  # Remove HTML, lower case\n",
    "jobs.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e619d5434d6e029d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "get vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ddab088120ac7e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = Doc2Vec.load('model/doc2vec_v3.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598193c1fbe866c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jobs['embd'] = jobs['content'].apply(lambda x: model.infer_vector(x.split()))\n",
    "jobs.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a22735b3d6925116"
  },
  {
   "cell_type": "markdown",
   "source": [
    "write vectors to FAISS index and store them. id's should match the jobs csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bee03537dce3be5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(80)  # size from model params in train_d2v\n",
    "print(index.is_trained)\n",
    "index.add(np.array(jobs.embd.values.tolist()))\n",
    "print(index.ntotal)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfa8b2661da95967"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"model/hh_v4en.index\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d66bc7298365275"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20c494e550eb016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "font_sizes = []\n",
    "weighted_text = []\n",
    "\n",
    "with pdfplumber.open('data/sample2.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        words = page.extract_words(x_tolerance=2, keep_blank_chars=True, use_text_flow=True, extra_attrs=[\"fontname\", \"size\"])\n",
    "        font_sizes.extend([float(word['size']) for word in words])\n",
    "\n",
    "    # Calculate median font size\n",
    "    median_size = np.median(font_sizes)\n",
    "    \n",
    "    # a bit of processing - some non-letter chars can be considered as spaces\n",
    "\n",
    "    for word in words:\n",
    "        word_text = word['text'].replace('/', ' ').replace('-', ' ').strip('.,').lower()\n",
    "        word_size = float(word['size'])\n",
    "\n",
    "        # Assign weight based on font size (example: 2x for each size unit above median)\n",
    "        if word_size > median_size:\n",
    "            weight = (word_size * 1.2 / median_size)\n",
    "        else:\n",
    "            weight = 1\n",
    "\n",
    "        # Replicate word based on weight\n",
    "        weighted_text.extend([word_text] * int(round(weight)))\n",
    "\n",
    "weighted_text = \" \".join([e for e in weighted_text if any(c.isalpha() for c in e)]).split()\n",
    "\n",
    "weighted_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5826116ddb431c0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resume = []\n",
    "\n",
    "with pdfplumber.open('data/sample2.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text(x_tolerance=2)\n",
    "        if text:\n",
    "            text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "            resume.append(text)\n",
    "\n",
    "resume = ' '.join([e.replace('\\n', ' ').lower() for e in resume])\n",
    "\n",
    "resume"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b75f64c49ed32d46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v1 = np.array([model.infer_vector(weighted_text)])\n",
    "\n",
    "# find the closest embedding in index\n",
    "distances, indices = index.search(v1, 5)\n",
    "distances, indices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "488a8f4795e8a67a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jobs.loc[indices[0]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12ddc0a226c3ee66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
