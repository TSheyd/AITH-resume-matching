{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "load additional packages for NLTK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e6bc551edaabc94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "914664e44ffea630"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rabota.ru jobs dataset was chosen for training - it's the largest one and the most diverse one probably (jobs are distributed all across Russia and IT is probably less prevalent here than on hh.ru)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd7a000403093fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/vacancy.csv', sep='|')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206d52cdf73054ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing - remove HTML tags, lowercase everything, remove punctuation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449402d01ac9373a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# HTML stripping (https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python)\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a16a753e7e360755"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['a_req', 'p_req', 'p_res']] = df[['additional_requirements', 'position_requirements' , 'position_responsibilities']].astype(str)\n",
    "df['total_req'] = df['a_req'] + ' ' + df['p_req'] + ' ' + df['p_res']\n",
    "df['total_req'] = df['total_req'].str.replace('nan', '').str.strip(' ')\n",
    "df['total_req'] = df['total_req'].apply(lambda x: re.sub(r'[^\\w\\s]', '', strip_tags(x.lower())))  # remove HTML tags, lower case, remove punctuation\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b10bd7dac012701"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df['total_req'].values.tolist()\n",
    "tagged_data = [TaggedDocument(words = nltk.tokenize.word_tokenize(_d), tags = [str(i)]) for i, _d in enumerate(data)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade2ffd4309b8daf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "init model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebe1c28f15821b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    vector_size = 50,\n",
    "    min_count = 10,\n",
    "    epochs = 50\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7538d59d9138d5d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "build vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23afb3fbf65bb20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)\n",
    "len(model.wv.key_to_index)  # vocab size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f323199f102ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b6a6306096ffb52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train(tagged_data,\n",
    "total_examples = model.corpus_count,\n",
    "epochs = model.epochs)\n",
    "model.save('model/doc2vec_v2.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddd9fa264e1025d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
